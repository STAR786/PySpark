{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d45ef159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting delta-spark\n",
      "  Downloading delta_spark-1.2.1-py3-none-any.whl (19 kB)\n",
      "Collecting pyspark<3.3.0,>=3.2.0\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "Requirement already satisfied: importlib-metadata>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from delta-spark) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.6.0)\n",
      "Collecting py4j==0.10.9.3\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=bd151a15cbc12a6edfbbfa6602cda73396fb42a145c7933ae460d9134a994808\n",
      "  Stored in directory: c:\\users\\syed3\\appdata\\local\\pip\\cache\\wheels\\52\\45\\50\\69db7b6e1da74a1b9fcc097827db9185cb8627117de852731e\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark, delta-spark\n",
      "Successfully installed delta-spark-1.2.1 py4j-0.10.9.3 pyspark-3.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1092ae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88ef3459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined view 'loans_delta'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"set spark.sql.shuffle.partitions = 1\")\n",
    "\n",
    "sourcePath = r\"C:\\Users\\syed3\\Downloads\\LearningSparkV2-master\\LearningSparkV2-master\\CSV's\\loans\\loan-risks.snappy.parquet\"\n",
    "\n",
    "# Configure Delta Lake Path\n",
    "deltaPath = r\"C:\\temp\"\n",
    "\n",
    "# # Remove folder if it exists\n",
    "# dbutils.fs.rm(deltaPath, recurse=True)\n",
    "\n",
    "# Create the Delta table with the same loans data\n",
    "(spark.read.format(\"parquet\").load(sourcePath) \n",
    "  .write.format(\"delta\").save(deltaPath))\n",
    "\n",
    "spark.read.format(\"delta\").load(deltaPath).createOrReplaceTempView(\"loans_delta\")\n",
    "print(\"Defined view 'loans_delta'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2347c4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   14705|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Loans row count\n",
    "spark.sql(\"SELECT count(*) FROM loans_delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54d07954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+\n",
      "|loan_id|funded_amnt|paid_amnt|addr_state|\n",
      "+-------+-----------+---------+----------+\n",
      "|      0|       1000|   182.22|        CA|\n",
      "|      1|       1000|   361.19|        WA|\n",
      "|      2|       1000|   176.26|        TX|\n",
      "|      3|       1000|   1000.0|        OK|\n",
      "|      4|       1000|   249.98|        PA|\n",
      "+-------+-----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM loans_delta LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4d03c8",
   "metadata": {},
   "source": [
    "### Loading Data Streams into a Delta Lake Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccfb297",
   "metadata": {},
   "outputs": [],
   "source": [
    "newLoanStreamDF = ... # Streaming DataFrame with new loans data\n",
    "checkpointDir = ... # Directory for streaming checkpoints\n",
    "streamingQuery = (newLoanStreamDF.writeStream\n",
    " .format(\"delta\")\n",
    " .option(\"checkpointLocation\", checkpointDir)\n",
    " .trigger(processingTime = \"10 seconds\")\n",
    " .start(deltaPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcfde75",
   "metadata": {},
   "source": [
    "###  Enforcing Schema on Write to Prevent Data Corruption\n",
    "The Delta Lake format records the schema as table-level metadata. Hence, all writes\n",
    "to a Delta Lake table can verify whether the data being written has a schema compati‚Äê\n",
    "ble with that of the table. If it is not compatible, Spark will throw an error before any\n",
    "data is written and committed to the table, thus preventing such accidental data corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "428e269f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "A schema mismatch detected when writing to the Delta table (Table ID: a526dc65-ff3d-44c1-b902-187eabd43e9e).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n\n\nData schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n-- closed: boolean (nullable = true)\n\n         ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6404/305175015.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m loanUpdates = (spark.createDataFrame(items, cols)\n\u001b[0;32m      8\u001b[0m  .withColumn(\"funded_amnt\", col(\"funded_amnt\").cast(\"int\")))\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mloanUpdates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"delta\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"append\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeltaPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.2.1-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.3-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.2.1-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: A schema mismatch detected when writing to the Delta table (Table ID: a526dc65-ff3d-44c1-b902-187eabd43e9e).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n\n\nData schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n-- closed: boolean (nullable = true)\n\n         "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "cols = ['loan_id', 'funded_amnt', 'paid_amnt', 'addr_state', 'closed']\n",
    "items = [\n",
    "(1111111, 1000, 1000.0, 'TX', True),\n",
    "(2222222, 2000, 0.0, 'CA', False)\n",
    "]\n",
    "loanUpdates = (spark.createDataFrame(items, cols)\n",
    " .withColumn(\"funded_amnt\", col(\"funded_amnt\").cast(\"int\")))\n",
    "loanUpdates.write.format(\"delta\").mode(\"append\").save(deltaPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58717b9",
   "metadata": {},
   "source": [
    "### Evolving Schemas to Accommodate Changing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02a204aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "(loanUpdates.write.format(\"delta\").mode(\"append\")\n",
    " .option(\"mergeSchema\", \"true\")\n",
    " .save(deltaPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826d0886",
   "metadata": {},
   "source": [
    "### #Updating data to fix errors\n",
    "#Suppose upon, reviewing the data, we realized that all of the loans assigned to addr_state = 'OR'\n",
    "#should have been assigned to addr_state = 'WA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9441558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|addr_state|count(1)|\n",
      "+----------+--------+\n",
      "|        CA|    2017|\n",
      "|        OR|     518|\n",
      "|        TX|    1295|\n",
      "|        NY|    1282|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data before updating\n",
    "spark.sql(\"\"\"SELECT addr_state, count(1) FROM loans_delta WHERE addr_state IN ('OR', 'WA', 'CA', 'TX', 'NY') GROUP BY addr_state\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16c97c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating the data\n",
    "from delta.tables import *\n",
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "deltaTable.update(\"addr_state = 'OR'\", {\"addr_state\": \"'WA'\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bb0123d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|addr_state|count(1)|\n",
      "+----------+--------+\n",
      "|        CA|    2017|\n",
      "|        WA|     518|\n",
      "|        TX|    1295|\n",
      "|        NY|    1282|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# after updating\n",
    "spark.sql(\"\"\"SELECT addr_state, count(1) FROM loans_delta WHERE addr_state IN ('OR', 'WA', 'CA', 'TX', 'NY') GROUP BY addr_state\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e98ca3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=deltaTable.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15ab0bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+------+\n",
      "|loan_id|funded_amnt|paid_amnt|addr_state|closed|\n",
      "+-------+-----------+---------+----------+------+\n",
      "|      0|       1000|   182.22|        CA|  null|\n",
      "|      1|       1000|   361.19|        WA|  null|\n",
      "|      2|       1000|   176.26|        TX|  null|\n",
      "|      3|       1000|   1000.0|        OK|  null|\n",
      "|      4|       1000|   249.98|        PA|  null|\n",
      "+-------+-----------+---------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f73159c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6be95cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    5134|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For user whose loans been paid-off\n",
    "spark.sql(\"SELECT COUNT(*) FROM loans_delta WHERE funded_amnt = paid_amnt\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3049052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del the user with fully paid loans\n",
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "deltaTable.delete(\"funded_amnt >= paid_amnt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9b82ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM loans_delta WHERE funded_amnt = paid_amnt\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5404c2",
   "metadata": {},
   "source": [
    "### Upserting change data to a table using merge\n",
    "A common use cases is Change Data Capture (CDC), where you have to replicate row changes made in an OLTP table to another table for OLAP workloads. To continue with our loan data example, say we have another table of new loan information, some of which are new loans and others are updates to existing loans. In addition, let‚Äôs say this changes table has the same schema as the loan_delta table. You can upsert these changes into the table using the DeltaTable.merge() operation which is based on the MERGE SQL command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "532471ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+\n",
      "|loan_id|funded_amnt|paid_amnt|addr_state|\n",
      "+-------+-----------+---------+----------+\n",
      "+-------+-----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from loans_delta where addr_state = 'NY' and loan_id < 30\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7c01b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"loan_id\", \"funded_amnt\", \"paid_amnt\", \"addr_state\", \"closed\"]\n",
    "\n",
    "items = [\n",
    "  (11, 1000, 1000.0, 'NY', True),   # loan paid off\n",
    "  (12, 1000, 0.0, 'NY', False)      # new loan\n",
    "]\n",
    "\n",
    "loanUpdates = spark.createDataFrame(items, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "698b9a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update the table with the changed data using the merge operation\n",
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "\n",
    "(deltaTable\n",
    "  .alias(\"t\")\n",
    "  .merge(loanUpdates.alias(\"s\"), \"t.loan_id = s.loan_id\") \n",
    "  .whenMatchedUpdateAll() \n",
    "  .whenNotMatchedInsertAll() \n",
    "  .execute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d802da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+\n",
      "|loan_id|funded_amnt|paid_amnt|addr_state|\n",
      "+-------+-----------+---------+----------+\n",
      "|     11|       1000|   1000.0|        NY|\n",
      "|     12|       1000|      0.0|        NY|\n",
      "+-------+-----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#UPDAted\n",
    "spark.sql(\"select * from loans_delta where addr_state = 'NY' and loan_id < 30\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da8e2b8",
   "metadata": {},
   "source": [
    "## Deduplicating data while inserting using insert-only merge\n",
    "The merge operation in Delta Lake supports an extended syntax beyond that specified by the ANSI standard. It supports advanced features like the following.\n",
    "\n",
    "- Delete actions: For example, MERGE ‚Ä¶ WHEN MATCHED THEN DELETE\n",
    "- Clause conditions: For example, `MERGE ‚Ä¶ WHEN MATCHED AND THEN ...``\n",
    "- Optional actions: All the MATCHED and NOT MATCHED clauses are optional.\n",
    "- Star syntax: For example, UPDATE * and INSERT * to update/insert all the columns in the target table with matching columns from the source dataset. The equivalent API in DeltaTable is updateAll() and insertAll(), which we have already seen.<br>\n",
    "\n",
    "This allows you to express many more complex use cases with little code. For example, say you want to backfill the loan_delta table with historical data of past loans. But some of the historical data may already have been inserted in the table and you don't want to update them (since their emails may already have been updated). You can deduplicate by the loan_id while inserting by running the following merge operation with only the INSERT action (since the UPDATE action is optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3097836e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+\n",
      "|loan_id|funded_amnt|paid_amnt|addr_state|\n",
      "+-------+-----------+---------+----------+\n",
      "|     11|       1000|   1000.0|        NY|\n",
      "|     12|       1000|      0.0|        NY|\n",
      "+-------+-----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from loans_delta where addr_state = 'NY' and loan_id < 30\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84b252",
   "metadata": {},
   "source": [
    "Let's say we have some historical data that we want to merge with this table. One of the historical loan exists in the current table but the historical table has old values, therefore it should not update the current value present in the table. And another historical does not exist in the current table, therefore it should be inserted into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1caf873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"loan_id\", \"funded_amnt\", \"paid_amnt\", \"addr_state\", \"closed\"]\n",
    "\n",
    "items = [\n",
    "  (11, 1000, 0.0, \"NY\", False),\n",
    "  (-100, 1000, 10.0, \"NY\", False)\n",
    "]\n",
    "\n",
    "historicalUpdates = spark.createDataFrame(items, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "097c65ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "\n",
    "(deltaTable\n",
    "  .alias(\"t\")\n",
    "  .merge(historicalUpdates.alias(\"s\"), \"t.loan_id = s.loan_id\") \n",
    "  .whenNotMatchedInsertAll() \n",
    "  .execute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77653cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+\n",
      "|loan_id|funded_amnt|paid_amnt|addr_state|\n",
      "+-------+-----------+---------+----------+\n",
      "|     11|       1000|   1000.0|        NY|\n",
      "|     12|       1000|      0.0|        NY|\n",
      "|   -100|       1000|     10.0|        NY|\n",
      "+-------+-----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from loans_delta where addr_state = 'NY' and loan_id < 30\").show()\n",
    "#Notice that the only change in the table is that insert of new loan, and existing loans were not updated to old values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "95217b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+------+\n",
      "|loan_id|funded_amnt|paid_amnt|addr_state|closed|\n",
      "+-------+-----------+---------+----------+------+\n",
      "|     11|       1000|   1000.0|        NY|  true|\n",
      "|     12|       1000|      0.0|        NY| false|\n",
      "|   -100|       1000|     10.0|        NY| false|\n",
      "+-------+-----------+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad500282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      7|2022-06-12 11:30:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|          6|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.2....|\n",
      "|      6|2022-06-12 11:14:...|  null|    null|    MERGE|{predicate -> (t....|null|    null|     null|          5|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.2....|\n",
      "|      5|2022-06-12 11:09:...|  null|    null|   DELETE|{predicate -> [\"(...|null|    null|     null|          4|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.2....|\n",
      "|      4|2022-06-12 11:07:...|  null|    null|   UPDATE|{predicate -> (ad...|null|    null|     null|          3|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.2....|\n",
      "|      3|2022-06-12 11:04:...|  null|    null|   UPDATE|{predicate -> (ad...|null|    null|     null|          2|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.2....|\n",
      "|      2|2022-06-12 10:54:...|  null|    null|   UPDATE|{predicate -> (ad...|null|    null|     null|          1|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.2....|\n",
      "|      1|2022-06-12 10:50:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|          0|  Serializable|         true|{numFiles -> 3, n...|        null|Apache-Spark/3.2....|\n",
      "|      0|2022-06-12 10:35:...|  null|    null|    WRITE|{mode -> ErrorIfE...|null|    null|     null|       null|  Serializable|         true|{numFiles -> 1, n...|        null|Apache-Spark/3.2....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d56b6351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|version|timestamp              |operation|operationParameters                                                                                                                      |\n",
      "+-------+-----------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|7      |2022-06-12 11:30:12.855|MERGE    |{predicate -> (t.loan_id = s.loan_id), matchedPredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}                       |\n",
      "|6      |2022-06-12 11:14:51.765|MERGE    |{predicate -> (t.loan_id = s.loan_id), matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|\n",
      "|5      |2022-06-12 11:09:07.989|DELETE   |{predicate -> [\"(CAST(funded_amnt AS DOUBLE) >= paid_amnt)\"]}                                                                            |\n",
      "|4      |2022-06-12 11:07:45.972|UPDATE   |{predicate -> (addr_state#3339 = OR)}                                                                                                    |\n",
      "+-------+-----------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.history(4).select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e75cae",
   "metadata": {},
   "source": [
    "# Querying previous snapshots of the table with time travel\n",
    "Delta Lake‚Äôs time travel feature allows you to access previous versions of the table. Here are some possible uses of this feature:\n",
    "\n",
    "Auditing Data Changes\n",
    "Reproducing experiments & reports\n",
    "Rollbacks\n",
    "You can query by using either a timestamp or a version number using Python, Scala, and/or SQL syntax. For this examples we will query a specific version using the Python syntax.\n",
    "\n",
    "For more information, refer to Introducing Delta Time Travel for Large Scale Data Lakes and the docs.\n",
    "\n",
    "Let's query the table's state before we deleted the data, which still contains the fully paid loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2fef83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    5134|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previousVersion = deltaTable.history(1).select(\"version\").first()[0] - 3\n",
    "\n",
    "(spark.read.format(\"delta\")\n",
    "  .option(\"versionAsOf\", previousVersion)\n",
    "  .load(deltaPath)\n",
    "  .createOrReplaceTempView(\"loans_delta_pre_delete\"))\n",
    "\n",
    "spark.sql(\"SELECT COUNT(*) FROM loans_delta_pre_delete WHERE funded_amnt = paid_amnt\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82291a88",
   "metadata": {},
   "source": [
    "# DATALAKES KEY OPERATIONS\n",
    "Transactional guarantees and schema management, like databases\n",
    "‚Ä¢ Scalability and openness, like data lakes\n",
    "\n",
    "‚Ä¢ Support for concurrent batch and streaming workloads with ACID guarantees\n",
    "\n",
    "‚Ä¢ Support for transformation of existing data using update, delete, and merge operations that ensure ACID guarantees\n",
    "\n",
    "‚Ä¢ Support for versioning, auditing of operation history, and querying of previous versions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
