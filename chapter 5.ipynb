{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd3301fe",
   "metadata": {},
   "source": [
    "## Spark SQL and DataFrames: Interacting with External Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a4d412",
   "metadata": {},
   "source": [
    "### In this chapter, we will focus on how Spark SQL interfaces with externalcomponents. Specifically, we discuss how Spark SQL allows you to:\n",
    "• Use user-defined functions for both Apache Hive and Apache Spark.<BR>\n",
    "• Connect with external data sources such as JDBC and SQL databases, Post‐\n",
    "greSQL, MySQL, Tableau, Azure Cosmos DB, and MS SQL Server.<BR>\n",
    "• Work with simple and complex types, higher-order functions, and common relational operators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a15ef92",
   "metadata": {},
   "source": [
    "### User Defined Functions\n",
    "While Apache Spark has a plethora of functions, the flexibility of Spark allows for data engineers and data scientists to define their own functions (i.e., user-defined functions or UDFs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b46436a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-8.0.0-cp39-cp39-win_amd64.whl (17.9 MB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyarrow) (1.20.3)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-8.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a76255e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"my_app\") .config(\"spark.memory.offHeap.enabled\",\"true\") .config(\"spark.memory.offHeap.size\",\"10g\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Create cubed function\n",
    "def cubed(s):\n",
    "    return s * s * s\n",
    "\n",
    "# Register UDF\n",
    "spark.udf.register(\"cubed\", cubed, LongType())\n",
    "\n",
    "# Generate temporary view\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348fd1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"cubed\", cubed, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb02a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe8ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b702a0e",
   "metadata": {},
   "source": [
    "# Evaluation order and null checking in Spark SQL\n",
    "Spark SQL (this includes SQL, the DataFrame API, and the Dataset API) does not\n",
    "guarantee the order of evaluation of subexpressions. For example, the following query\n",
    "does not guarantee that the s is NOT NULL clause is executed prior to the strlen(s) > 1 clause:\n",
    "> spark.sql(\"SELECT s FROM test1 WHERE s IS NOT NULL AND strlen(s) > 1\")\n",
    "\n",
    "\n",
    "Therefore, to perform proper null checking, it is recommended that you do the\n",
    "following:\n",
    "1. Make the UDF itself null-aware and do null checking inside the UDF.\n",
    "2. Use IF or CASE WHEN expressions to do the null check and invoke the UDF in a\n",
    "conditional branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2452b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the cubed function \n",
    "def cubed(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a\n",
    "\n",
    "# Create the pandas UDF for the cubed function \n",
    "cubed_udf = pandas_udf(cubed, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4956c155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     8\n",
      "2    27\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Using pandas dataframe\n",
    "# Create a Pandas series\n",
    "x = pd.Series([1, 2, 3])\n",
    "\n",
    "# The function for a pandas_udf executed with local Pandas data\n",
    "print(cubed(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "386ffe53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|cubed(id)|\n",
      "+---+---------+\n",
      "|  1|        1|\n",
      "|  2|        8|\n",
      "|  3|       27|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark DataFrame\n",
    "df = spark.range(1, 4)\n",
    "\n",
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(\"id\", cubed_udf(col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea6be9e",
   "metadata": {},
   "source": [
    "# Higher Order Functions in DataFrames and Spark SQL\n",
    "Because complex data types are an amalgamation of simple data types, it is tempting to manipulate complex data types directly. As noted in the post Introducing New Built-in and Higher-Order Functions for Complex Data Types in Apache Spark 2.4 there are typically two solutions for the manipulation of complex data types.\n",
    "\n",
    "Exploding the nested structure into individual rows, applying some function, and then re-creating the nested structure as noted in the code snippet below (see Option 1)\n",
    "Building a User Defined Function (UDF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7fb6278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- values: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n",
      "+---+---------+\n",
      "| id|   values|\n",
      "+---+---------+\n",
      "|  1|[1, 2, 3]|\n",
      "|  2|[2, 3, 4]|\n",
      "|  3|[3, 4, 5]|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an array dataset\n",
    "arrayData = [[1, (1, 2, 3)], [2, (2, 3, 4)], [3, (3, 4, 5)]]\n",
    "\n",
    "# Create schema\n",
    "from pyspark.sql.types import *\n",
    "#created the schema\n",
    "arraySchema = (StructType([\n",
    "      StructField(\"id\", IntegerType(), True), \n",
    "      StructField(\"values\", ArrayType(IntegerType()), True)\n",
    "      ]))\n",
    "\n",
    "# Create DataFrame on the basis of schema\n",
    "df = spark.createDataFrame(spark.sparkContext.parallelize(arrayData), arraySchema)\n",
    "df.createOrReplaceTempView(\"table\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba032e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  1|    1|\n",
      "|  1|    2|\n",
      "|  1|    3|\n",
      "|  2|    2|\n",
      "|  2|    3|\n",
      "|  2|    4|\n",
      "|  3|    3|\n",
      "|  3|    4|\n",
      "|  3|    5|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In this nested SQL statement, we first explode(values) which creates a new row (with the id) for each element (value) within values\n",
    "spark.sql('SELECT id, explode(values) AS value FROM table').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4191b898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|newValues|\n",
      "+---+---------+\n",
      "|  1|[2, 3, 4]|\n",
      "|  2|[3, 4, 5]|\n",
      "|  3|[4, 5, 6]|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT id, collect_list(value + 1) AS newValues\n",
    "  FROM  (SELECT id, explode(values) AS value\n",
    "        FROM table) x\n",
    " GROUP BY id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25db702",
   "metadata": {},
   "source": [
    "# Option 2: User Defined Function\n",
    "To perform the same task (adding a value of 1 to each element in values), we can also create a user defined function (UDF) that uses map to iterate through each element (value) to perform the addition operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf9aea59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|   values|\n",
      "+---+---------+\n",
      "|  1|[2, 3, 4]|\n",
      "|  2|[3, 4, 5]|\n",
      "|  3|[4, 5, 6]|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "# Create UDF\n",
    "def addOne(values):\n",
    "    return [value + 1 for value in values]\n",
    "\n",
    "# Register UDF\n",
    "spark.udf.register(\"plusOneIntPy\", addOne, ArrayType(IntegerType()))  \n",
    "\n",
    "# Query data\n",
    "spark.sql(\"SELECT id, plusOneIntPy(values) AS values FROM table\").show()\n",
    "# see its way more easier than above snippit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23943f4",
   "metadata": {},
   "source": [
    "# Higher-Order Functions\n",
    "In addition to the previously noted built-in functions, there are high-order functions that take anonymous lambda functions as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bfe0d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             celsius|\n",
      "+--------------------+\n",
      "|[35, 36, 32, 30, ...|\n",
      "|[31, 32, 34, 55, 56]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))])\n",
    "\n",
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n",
    "t_c = spark.createDataFrame(t_list, schema)\n",
    "t_c.createOrReplaceTempView(\"tC\")\n",
    "\n",
    "# Show the DataFrame\n",
    "t_c.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f75ada",
   "metadata": {},
   "source": [
    "# Transform\n",
    "transform(array<T>, function<T, U>): array<U>\n",
    "\n",
    "The transform function produces an array by applying a function to each element of an input array (similar to a map function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53597969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             celsius|          fahrenheit|\n",
      "+--------------------+--------------------+\n",
      "|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
      "|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Fahrenheit from Celsius for an array of temperatures\n",
    "spark.sql(\n",
    "    \"\"\"SELECT celsius, transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39546911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             celsius|    high|\n",
      "+--------------------+--------+\n",
      "|[35, 36, 32, 30, ...|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]|[55, 56]|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter temperatures > 38C for array of temperatures\n",
    "spark.sql(\"\"\"SELECT celsius, filter(celsius, t -> t > 38) as high FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a53b8fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             celsius|threshold|\n",
      "+--------------------+---------+\n",
      "|[35, 36, 32, 30, ...|     true|\n",
      "|[31, 32, 34, 55, 56]|    false|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Is there a temperature of 38C in the array of temperatures\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, exists(celsius, t -> t = 38) as threshold\n",
    "FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d096b",
   "metadata": {},
   "source": [
    "# DataFrames and Spark SQL Common Relational Operators\n",
    "The power of Spark SQL is that it contains many DataFrame Operations (also known as Untyped Dataset Operations).\n",
    "\n",
    "In the next section, we will focus on the following common relational operators:\n",
    "\n",
    "Unions and Joins, \n",
    "Windowing, \n",
    "Modifications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0e83ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Set File Paths\n",
    "delays_path = r\"C:\\Users\\syed3\\Downloads\\LearningSparkV2-master\\LearningSparkV2-master\\databricks-datasets\\learning-spark-v2\\flights\\departuredelays.csv\"\n",
    "airports_path = r\"C:\\Users\\syed3\\Downloads\\LearningSparkV2-master\\LearningSparkV2-master\\databricks-datasets\\learning-spark-v2\\flights\\airport-codes-na.txt\"\n",
    "\n",
    "# Obtain airports dataset\n",
    "airports = spark.read.options(header=\"true\", inferSchema=\"true\", sep=\"\\t\").csv(airports_path)\n",
    "airports.createOrReplaceTempView(\"airports_na\")\n",
    "\n",
    "# Obtain departure Delays data\n",
    "delays = spark.read.options(header=\"true\").csv(delays_path)\n",
    "delays = (delays\n",
    "          .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    "          .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\")))\n",
    "\n",
    "delays.createOrReplaceTempView(\"departureDelays\")\n",
    "\n",
    "# Create temporary small table\n",
    "foo = delays.filter(expr(\"\"\" \n",
    "            origin == 'SEA' AND \n",
    "            destination == 'SFO' AND \n",
    "            date like '01010%' AND \n",
    "            delay > 0\"\"\")) \n",
    "\n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3355cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82ab8515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff82345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM foo LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6409b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Union two tables\n",
    "bar = delays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bars\")\n",
    "bar.filter(expr(\"origin == 'SEA' AND destination == 'SFO' AND date LIKE '01010%' AND delay > 0\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c26a53fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * \n",
    "FROM bars\n",
    "WHERE origin = 'SEA' \n",
    "   AND destination = 'SFO' \n",
    "   AND date LIKE '01010%' \n",
    "   AND delay > 0\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7d7e83d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+-----+-------+----+\n",
      "|    date|delay|distance|origin|destination|   City|State|Country|IATA|\n",
      "+--------+-----+--------+------+-----------+-------+-----+-------+----+\n",
      "|01010710|   31|     590|   SEA|        SFO|Seattle|   WA|    USA| SEA|\n",
      "|01010955|  104|     590|   SEA|        SFO|Seattle|   WA|    USA| SEA|\n",
      "|01010730|    5|     590|   SEA|        SFO|Seattle|   WA|    USA| SEA|\n",
      "+--------+-----+--------+------+-----------+-------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join Departure Delays data (foo) with flight info\n",
    "foo.join(\n",
    "  airports, \n",
    "  airports.IATA == foo.origin\n",
    ").select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cee27983",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('DROP TABLE IF EXISTS departureDelaysWindow')\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE departureDelaysWindow AS\n",
    "SELECT origin, destination, sum(delay) as TotalDelays \n",
    "  FROM departureDelays \n",
    " WHERE origin IN ('SEA', 'SFO', 'JFK') \n",
    "   AND destination IN ('SEA', 'SFO', 'JFK', 'DEN', 'ORD', 'LAX', 'ATL') \n",
    " GROUP BY origin, destination\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"SELECT * FROM departureDelaysWindow\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d06410",
   "metadata": {},
   "source": [
    "# Modifications\n",
    "Another common DataFrame operation is to perform modifications to the DataFrame. Recall that the underlying RDDs are immutable (i.e. they do not change) to ensure there is data lineage for Spark operations. Hence while DataFrames themselves are immutable, you can modify them through operations that create a new, different DataFrame with different columns, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b90f0273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|On-time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo2 = foo.withColumn(\"status\", expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\"))\n",
    "foo2.show()\n",
    "#The newly created foo2 DataFrame has the contents of the original foo DataFrame\n",
    "#plus the additional status column defined by the CASE statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4069067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------+\n",
      "|    date|distance|origin|destination| status|\n",
      "+--------+--------+------+-----------+-------+\n",
      "|01010710|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|     590|   SEA|        SFO|On-time|\n",
      "+--------+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "856de132",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------------+\n",
      "|    date|distance|origin|destination|flight_status|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "|01010710|     590|   SEA|        SFO|      Delayed|\n",
      "|01010955|     590|   SEA|        SFO|      Delayed|\n",
      "|01010730|     590|   SEA|        SFO|      On-time|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#rename\n",
    "foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6560662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|date    |delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|6    |602     |ABE   |ATL        |\n",
      "|01020600|-8   |369     |ABE   |DTW        |\n",
      "|01021245|-2   |602     |ABE   |ATL        |\n",
      "|01020605|-4   |602     |ABE   |ATL        |\n",
      "|01031245|-4   |602     |ABE   |ATL        |\n",
      "|01030605|0    |602     |ABE   |ATL        |\n",
      "|01041243|10   |602     |ABE   |ATL        |\n",
      "|01040605|28   |602     |ABE   |ATL        |\n",
      "|01051245|88   |602     |ABE   |ATL        |\n",
      "|01050605|9    |602     |ABE   |ATL        |\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''select * from departureDelays''').show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2b5d292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----+\n",
      "|destination|month|delay|\n",
      "+-----------+-----+-----+\n",
      "|        ORD|    1|   92|\n",
      "|        JFK|    1|   -7|\n",
      "|        DFW|    1|   -5|\n",
      "|        MIA|    1|   -3|\n",
      "|        DFW|    1|   -3|\n",
      "|        DFW|    1|    1|\n",
      "|        ORD|    1|  -10|\n",
      "|        DFW|    1|   -6|\n",
      "|        DFW|    1|   -2|\n",
      "|        ORD|    1|   -3|\n",
      "+-----------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#When working with your data, sometimes you will need to swap the columns for the rows—i.e., pivot your data.\n",
    "spark.sql(\n",
    "\"\"\"SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay \n",
    "FROM departureDelays WHERE origin = 'SEA'\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f1ec0",
   "metadata": {},
   "source": [
    "Pivoting allows you to place names in the month column (instead of 1 and 2 you can\n",
    "show Jan and Feb, respectively) as well as perform aggregate calculations (in this case\n",
    "average and max) on the delays by destination and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "641af200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+------------+------------+------------+\n",
      "|destination|JAN_AvgDelay|JAN_MaxDelay|FEB_AvgDelay|FEB_MaxDelay|MAR_AvgDelay|MAR_MaxDelay|\n",
      "+-----------+------------+------------+------------+------------+------------+------------+\n",
      "|        ABQ|       19.86|         316|       11.42|          69|       11.47|          74|\n",
      "|        ANC|        4.44|         149|        7.90|         141|        5.10|         187|\n",
      "|        ATL|       11.98|         397|        7.73|         145|        6.53|         109|\n",
      "|        AUS|        3.48|          50|       -0.21|          18|        4.03|          61|\n",
      "|        BOS|        7.84|         110|       14.58|         152|        7.78|         119|\n",
      "|        BUR|       -2.03|          56|       -1.89|          78|        2.01|         108|\n",
      "|        CLE|       16.00|          27|        null|        null|        null|        null|\n",
      "|        CLT|        2.53|          41|       12.96|         228|        5.16|         110|\n",
      "|        COS|        5.32|          82|       12.18|         203|        9.74|         205|\n",
      "|        CVG|       -0.50|           4|        null|        null|        null|        null|\n",
      "|        DCA|       -1.15|          50|        0.07|          34|        5.73|         199|\n",
      "|        DEN|       13.13|         425|       12.95|         625|        7.48|         231|\n",
      "|        DFW|        7.95|         247|       12.57|         356|        6.71|         277|\n",
      "|        DTW|        9.18|         107|        3.47|          77|        2.47|          72|\n",
      "|        EWR|        9.63|         236|        5.20|         212|       10.59|         181|\n",
      "|        FAI|        1.84|         160|        4.21|          60|        5.32|          98|\n",
      "|        FAT|        1.36|         119|        5.22|         232|        1.67|          92|\n",
      "|        FLL|        2.94|          54|        3.50|          40|        3.06|          52|\n",
      "|        GEG|        2.28|          63|        2.87|          60|        4.49|          89|\n",
      "|        HDN|       -0.44|          27|       -6.50|           0|       -3.44|          15|\n",
      "+-----------+------------+------------+------------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM (\n",
    "SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay \n",
    "  FROM departureDelays WHERE origin = 'SEA' \n",
    ") \n",
    "PIVOT (\n",
    "  CAST(AVG(delay) AS DECIMAL(4, 2)) as AvgDelay, MAX(delay) as MaxDelay\n",
    "  FOR month IN (1 JAN, 2 FEB, 3 MAR)\n",
    ")\n",
    "ORDER BY destination\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c87da7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+------------+\n",
      "|destination|JAN_AvgDelay|JAN_MaxDelay|FEB_AvgDelay|FEB_MaxDelay|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "|        ABQ|       19.86|         316|       11.42|          69|\n",
      "|        ANC|        4.44|         149|        7.90|         141|\n",
      "|        ATL|       11.98|         397|        7.73|         145|\n",
      "|        AUS|        3.48|          50|       -0.21|          18|\n",
      "|        BOS|        7.84|         110|       14.58|         152|\n",
      "|        BUR|       -2.03|          56|       -1.89|          78|\n",
      "|        CLE|       16.00|          27|        null|        null|\n",
      "|        CLT|        2.53|          41|       12.96|         228|\n",
      "|        COS|        5.32|          82|       12.18|         203|\n",
      "|        CVG|       -0.50|           4|        null|        null|\n",
      "|        DCA|       -1.15|          50|        0.07|          34|\n",
      "|        DEN|       13.13|         425|       12.95|         625|\n",
      "|        DFW|        7.95|         247|       12.57|         356|\n",
      "|        DTW|        9.18|         107|        3.47|          77|\n",
      "|        EWR|        9.63|         236|        5.20|         212|\n",
      "|        FAI|        1.84|         160|        4.21|          60|\n",
      "|        FAT|        1.36|         119|        5.22|         232|\n",
      "|        FLL|        2.94|          54|        3.50|          40|\n",
      "|        GEG|        2.28|          63|        2.87|          60|\n",
      "|        HDN|       -0.44|          27|       -6.50|           0|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM (\n",
    "SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay \n",
    "  FROM departureDelays WHERE origin = 'SEA' \n",
    ") \n",
    "PIVOT (\n",
    "  CAST(AVG(delay) AS DECIMAL(4, 2)) as AvgDelay, MAX(delay) as MaxDelay\n",
    "  FOR month IN (1 JAN, 2 FEB)\n",
    ")\n",
    "ORDER BY destination\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b7ee659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how Spark manages memory might help you understand the underlying concept"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
