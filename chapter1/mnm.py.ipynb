{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "341f454b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State|Color |Count|\n",
      "+-----+------+-----+\n",
      "|TX   |Red   |20   |\n",
      "|NV   |Blue  |66   |\n",
      "|CO   |Blue  |79   |\n",
      "|OR   |Blue  |71   |\n",
      "|WA   |Yellow|93   |\n",
      "+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+------+----------+\n",
      "|State|Color |sum(Count)|\n",
      "+-----+------+----------+\n",
      "|CA   |Yellow|100956    |\n",
      "|WA   |Green |96486     |\n",
      "|CA   |Brown |95762     |\n",
      "|TX   |Green |95753     |\n",
      "|TX   |Red   |95404     |\n",
      "|CO   |Yellow|95038     |\n",
      "|NM   |Red   |94699     |\n",
      "|OR   |Orange|94514     |\n",
      "|WY   |Green |94339     |\n",
      "|NV   |Orange|93929     |\n",
      "|TX   |Yellow|93819     |\n",
      "|CO   |Green |93724     |\n",
      "|CO   |Brown |93692     |\n",
      "|CA   |Green |93505     |\n",
      "|NM   |Brown |93447     |\n",
      "|CO   |Blue  |93412     |\n",
      "|WA   |Red   |93332     |\n",
      "|WA   |Brown |93082     |\n",
      "|WA   |Yellow|92920     |\n",
      "|NM   |Yellow|92747     |\n",
      "|NV   |Brown |92478     |\n",
      "|TX   |Orange|92315     |\n",
      "|AZ   |Brown |92287     |\n",
      "|AZ   |Green |91882     |\n",
      "|WY   |Red   |91768     |\n",
      "|AZ   |Orange|91684     |\n",
      "|CA   |Red   |91527     |\n",
      "|WA   |Orange|91521     |\n",
      "|NV   |Yellow|91390     |\n",
      "|UT   |Orange|91341     |\n",
      "|NV   |Green |91331     |\n",
      "|NM   |Orange|91251     |\n",
      "|NM   |Green |91160     |\n",
      "|WY   |Blue  |91002     |\n",
      "|UT   |Red   |90995     |\n",
      "|CO   |Orange|90971     |\n",
      "|AZ   |Yellow|90946     |\n",
      "|TX   |Brown |90736     |\n",
      "|OR   |Blue  |90526     |\n",
      "|CA   |Orange|90311     |\n",
      "|OR   |Red   |90286     |\n",
      "|NM   |Blue  |90150     |\n",
      "|AZ   |Red   |90042     |\n",
      "|NV   |Blue  |90003     |\n",
      "|UT   |Blue  |89977     |\n",
      "|AZ   |Blue  |89971     |\n",
      "|WA   |Blue  |89886     |\n",
      "|OR   |Green |89578     |\n",
      "|CO   |Red   |89465     |\n",
      "|NV   |Red   |89346     |\n",
      "|UT   |Yellow|89264     |\n",
      "|OR   |Brown |89136     |\n",
      "|CA   |Blue  |89123     |\n",
      "|UT   |Brown |88973     |\n",
      "|TX   |Blue  |88466     |\n",
      "|UT   |Green |88392     |\n",
      "|OR   |Yellow|88129     |\n",
      "|WY   |Orange|87956     |\n",
      "|WY   |Yellow|87800     |\n",
      "|WY   |Brown |86110     |\n",
      "+-----+------+----------+\n",
      "\n",
      "Total Rows = 60\n",
      "+-----+------+----------+\n",
      "|State|Color |sum(Count)|\n",
      "+-----+------+----------+\n",
      "|CA   |Yellow|100956    |\n",
      "|CA   |Brown |95762     |\n",
      "|CA   |Green |93505     |\n",
      "|CA   |Red   |91527     |\n",
      "|CA   |Orange|90311     |\n",
      "|CA   |Blue  |89123     |\n",
      "+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"PythonMnMCount\")\n",
    "    .getOrCreate())\n",
    "# get the M&M data set file name\n",
    "mnm_file = r'C:\\Users\\syed3\\Downloads\\LearningSparkV2-master\\LearningSparkV2-master\\chapter2\\py\\src\\data\\mnm_dataset.csv'\n",
    "# read the file into a Spark DataFrame\n",
    "mnm_df = (spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(mnm_file))\n",
    "mnm_df.show(n=5, truncate=False)\n",
    "\n",
    "# aggregate count of all colors and groupBy state and color\n",
    "# orderBy descending order\n",
    "count_mnm_df = (mnm_df.select(\"State\", \"Color\", \"Count\")\n",
    "                .groupBy(\"State\", \"Color\")\n",
    "                .sum(\"Count\")\n",
    "                .orderBy(\"sum(Count)\", ascending=False))\n",
    "\n",
    "# show all the resulting aggregation for all the dates and colors\n",
    "count_mnm_df.show(n=60, truncate=False)\n",
    "print(\"Total Rows = %d\" % (count_mnm_df.count()))\n",
    "\n",
    "# find the aggregate count for California by filtering\n",
    "ca_count_mnm_df = (mnm_df.select(\"*\")\n",
    "                   .where(mnm_df.State == 'CA')\n",
    "                   .groupBy(\"State\", \"Color\")\n",
    "                   .sum(\"Count\")\n",
    "                   .orderBy(\"sum(Count)\", ascending=False))\n",
    "\n",
    "# show the resulting aggregation for California\n",
    "ca_count_mnm_df.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff1778be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Brooke|    22.5|\n",
      "| Denny|    31.0|\n",
      "| Jules|    30.0|\n",
      "|    TD|    35.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "# Create a DataFrame using SparkSession\n",
    "spark = (SparkSession\n",
    " .builder\n",
    " .appName(\"AuthorsAges\")\n",
    " .getOrCreate())\n",
    "# Create a DataFrame\n",
    "data_df = spark.createDataFrame([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),\n",
    "(\"TD\", 35), (\"Brooke\", 25)], [\"name\", \"age\"])\n",
    "# Group the same names together, aggregate their ages, and compute an average\n",
    "avg_df = data_df.groupBy(\"name\").agg(avg(\"age\"))\n",
    "# Show the results of the final execution\n",
    "avg_df.show()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c645df5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Define schema for our data using DDL\n",
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING,`Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n",
    "# Create our static data\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"LinkedIn\"]],\n",
    " [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\"LinkedIn\"]],\n",
    " [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    " [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,[\"twitter\", \"FB\"]],\n",
    " [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    " [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568,[\"twitter\", \"LinkedIn\"]]\n",
    " ]\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    " # Create a SparkSession\n",
    "    spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"Example-3_6\")\n",
    "    .getOrCreate())\n",
    "    # Create a DataFrame using the schema defined above\n",
    "    blogs_df = spark.createDataFrame(data, schema)\n",
    "    # Show the DataFrame; it should reflect our table above\n",
    "    blogs_df.show()\n",
    "    # Print the schema used by Spark to process the DataFrame\n",
    "    print(blogs_df.printSchema())\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db35ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e3073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
