{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8929db8",
   "metadata": {},
   "source": [
    "# Optimizing and Tuning Spark Applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62890676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"my_app\") .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceb35ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+----------------------------------------------------------------+\n",
      "|key                                                      |value                                                           |\n",
      "+---------------------------------------------------------+----------------------------------------------------------------+\n",
      "|spark.sql.adaptive.advisoryPartitionSizeInBytes          |<value of spark.sql.adaptive.shuffle.targetPostShuffleInputSize>|\n",
      "|spark.sql.adaptive.autoBroadcastJoinThreshold            |<undefined>                                                     |\n",
      "|spark.sql.adaptive.coalescePartitions.enabled            |true                                                            |\n",
      "|spark.sql.adaptive.coalescePartitions.initialPartitionNum|<undefined>                                                     |\n",
      "|spark.sql.adaptive.coalescePartitions.minPartitionSize   |1MB                                                             |\n",
      "+---------------------------------------------------------+----------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SET -v\").select(\"key\", \"value\").show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d631fc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e33c9d",
   "metadata": {},
   "source": [
    "# dynamic resource allocation\n",
    "When you specify compute resources as command-line arguments to spark-submit, you cap the limit. This means that if more resources are needed later\n",
    "as tasks queue up in the driver due to a larger than anticipated workload, Spark can‚Äê\n",
    "not accommodate or allocate extra resources.\n",
    "To enable and configure dynamic allocation, you can use settings like the following.\n",
    "the numbers here are arbitrary; the appropriate settings will depend on the\n",
    "nature of your workload and they should be adjusted accordingly. Some of these\n",
    "configs cannot be set inside a Spark CLI, so you will have to set them\n",
    "programmatically:\n",
    "<BR>\n",
    "    \n",
    "    \n",
    "spark.dynamicAllocation.enabled true\n",
    "spark.dynamicAllocation.minExecutors 2\n",
    "spark.dynamicAllocation.schedulerBacklogTimeout 1m\n",
    "spark.dynamicAllocation.maxExecutors 20\n",
    "spark.dynamicAllocation.executorIdleTimeout 2min\n",
    "<br>\n",
    "    \n",
    "    \n",
    "By default spark.dynamicAllocation.enabled is set to false. When enabled with\n",
    "the settings shown here, the Spark driver will request that the cluster manager create\n",
    "two executors to start with, as a minimum (spark.dynamicAllocation.minExecu\n",
    "tors). As the task queue backlog increases, new executors will be requested each time\n",
    "the backlog timeout (spark.dynamicAllocation.schedulerBacklogTimeout) is\n",
    "exceeded. In this case, whenever there are pending tasks that have not been scheduled\n",
    "for over 1 minute, the driver will request that a new executor be launched to schedule\n",
    "backlogged tasks, up to a maximum of 20 (spark.dynamicAllocation.maxExecu\n",
    "tors). By contrast, if an executor finishes a task and is idle for 2 minutes\n",
    "(spark.dynamicAllocation.executorIdleTimeout), the Spark driver will terminate\n",
    "it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fb8575",
   "metadata": {},
   "source": [
    "# caching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27b9ea37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a large data set with couple of columns\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = spark.range(1 * 1000).toDF(\"id\").withColumn(\"square\", col(\"id\") * col(\"id\"))\n",
    "df.cache().count()\n",
    "#Check the Spark UI storage tab to see where the data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69b5bd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, square: bigint]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you do not unpersist, df2 below will not be cached because it has the same query plan as df\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe66d0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use persist(StorageLevel.Level)\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "df2 = spark.range(1 * 1000).toDF(\"id\").withColumn(\"square\", col(\"id\") * col(\"id\"))\n",
    "df2.persist(StorageLevel.DISK_ONLY).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8906e58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, square: bigint]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57822dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "spark.sql(\"CACHE TABLE dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f1a30c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) FROM dfTable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f839a44b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16324/2218380179.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453af21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
