{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3900658",
   "metadata": {},
   "source": [
    "### In this section, you will know how to create temporary views and tablesfrom the existing built-in data sources.Whether youâ€™re using the DataFrame API or SQL, the queries produce identical outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8b2bdf",
   "metadata": {},
   "source": [
    "## This notebook shows how to use SQL on a US Flights Dataset dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ffd6e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"flight_analysis\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee333a99",
   "metadata": {},
   "source": [
    "### Define a UDF to convert the date format into a readable format.\n",
    "\n",
    "Note: the date is a string with year missing, so it might be difficult to do any queries using SQL year() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78af8854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02/19  09:25'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_date_format_udf(d_str):\n",
    "  l = [char for char in d_str]\n",
    "  return \"\".join(l[0:2]) + \"/\" +  \"\".join(l[2:4]) + \" \" + \" \" +\"\".join(l[4:6]) + \":\" + \"\".join(l[6:])\n",
    "\n",
    "to_date_format_udf(\"02190925\")# how timestamps are in data to Mm/Yy time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99882fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.to_date_format_udf(d_str)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"to_date_format_udf\", to_date_format_udf, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87178ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, delay: int, distance: int, origin: string, destination: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Read our US departure flight data\n",
    "df = (spark.read.format(\"csv\")\n",
    "      .schema(\"date STRING, delay INT, distance INT, origin STRING, destination STRING\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"path\", r\"C:\\Users\\syed3\\Downloads\\LearningSparkV2-master\\LearningSparkV2-master\\databricks-datasets\\learning-spark-v2\\flights\\departuredelays.csv\")\n",
    "      .load())\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6cb6c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|data_format |\n",
      "+------------+\n",
      "|01/01  12:45|\n",
      "|01/02  06:00|\n",
      "|01/02  12:45|\n",
      "|01/02  06:05|\n",
      "|01/03  12:45|\n",
      "|01/03  06:05|\n",
      "|01/04  12:43|\n",
      "|01/04  06:05|\n",
      "|01/05  12:45|\n",
      "|01/05  06:05|\n",
      "+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"to_date_format_udf(date) as data_format\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6f157a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a temporary view to which we can issue SQL queries\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7080bd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|date    |delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|6    |602     |ABE   |ATL        |\n",
      "|01020600|-8   |369     |ABE   |DTW        |\n",
      "|01021245|-2   |602     |ABE   |ATL        |\n",
      "|01020605|-4   |602     |ABE   |ATL        |\n",
      "|01031245|-4   |602     |ABE   |ATL        |\n",
      "|01030605|0    |602     |ABE   |ATL        |\n",
      "|01041243|10   |602     |ABE   |ATL        |\n",
      "|01040605|28   |602     |ABE   |ATL        |\n",
      "|01051245|88   |602     |ABE   |ATL        |\n",
      "|01050605|9    |602     |ABE   |ATL        |\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from us_delay_flights_tbl ').show(10, truncate =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd849008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+--------+------------+\n",
      "|date    |delay|distance|origin|destination|date    |date_fm     |\n",
      "+--------+-----+--------+------+-----------+--------+------------+\n",
      "|01011245|6    |602     |ABE   |ATL        |01011245|01/01  12:45|\n",
      "|01020600|-8   |369     |ABE   |DTW        |01020600|01/02  06:00|\n",
      "|01021245|-2   |602     |ABE   |ATL        |01021245|01/02  12:45|\n",
      "|01020605|-4   |602     |ABE   |ATL        |01020605|01/02  06:05|\n",
      "|01031245|-4   |602     |ABE   |ATL        |01031245|01/03  12:45|\n",
      "|01030605|0    |602     |ABE   |ATL        |01030605|01/03  06:05|\n",
      "|01041243|10   |602     |ABE   |ATL        |01041243|01/04  12:43|\n",
      "|01040605|28   |602     |ABE   |ATL        |01040605|01/04  06:05|\n",
      "|01051245|88   |602     |ABE   |ATL        |01051245|01/05  12:45|\n",
      "|01050605|9    |602     |ABE   |ATL        |01050605|01/05  06:05|\n",
      "+--------+-----+--------+------+-----------+--------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert all date to date_fm so it's more convenient\n",
    "\n",
    "spark.sql(\"SELECT *, date, to_date_format_udf(date) AS date_fm FROM us_delay_flights_tbl\").show(\n",
    "    10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f8cba0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|4330    |HNL   |JFK        |\n",
      "|4330    |HNL   |JFK        |\n",
      "|4330    |HNL   |JFK        |\n",
      "|4330    |HNL   |JFK        |\n",
      "|4330    |HNL   |JFK        |\n",
      "|4330    |HNL   |JFK        |\n",
      "|4330    |HNL   |JFK        |\n",
      "|4330    |HNL   |JFK        |\n",
      "|4330    |HNL   |JFK        |\n",
      "|4330    |HNL   |JFK        |\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find out all flights whose distance between origin and destination is greater than 1000\n",
    "spark.sql(\n",
    "\"SELECT distance, origin, destination FROM us_delay_flights_tbl WHERE distance > 1000 ORDER BY distance DESC\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e376a7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|4330    |HNL   |JFK        |\n",
      "|4330    |HNL   |JFK        |\n",
      "|4330    |HNL   |JFK        |\n",
      "|4330    |HNL   |JFK        |\n",
      "|4330    |HNL   |JFK        |\n",
      "|4330    |HNL   |JFK        |\n",
      "|4330    |HNL   |JFK        |\n",
      "|4330    |HNL   |JFK        |\n",
      "|4330    |HNL   |JFK        |\n",
      "|4330    |HNL   |JFK        |\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query in data frame\n",
    "#df.select(\"distance\", \"origin\", \"destination\").where(\"distance > 1000\").orderBy(\"distance\", ascending=False).show(10)\n",
    "df.select(\n",
    "\"distance\", \"origin\", \"destination\").where(col(\"distance\") > 1000).orderBy(desc(\"distance\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1da930cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|date    |delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|02190925|1638 |SFO   |ORD        |\n",
      "|01031755|396  |SFO   |ORD        |\n",
      "|01022330|326  |SFO   |ORD        |\n",
      "|01051205|320  |SFO   |ORD        |\n",
      "|01190925|297  |SFO   |ORD        |\n",
      "|02171115|296  |SFO   |ORD        |\n",
      "|01071040|279  |SFO   |ORD        |\n",
      "|01051550|274  |SFO   |ORD        |\n",
      "|03120730|266  |SFO   |ORD        |\n",
      "|01261104|258  |SFO   |ORD        |\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find out all flights with 2 hour delays between San Francisco and Chicago\n",
    "spark.sql(\"\"\"\n",
    "SELECT date, delay, origin, destination \n",
    "FROM us_delay_flights_tbl \n",
    "WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD' \n",
    "ORDER by delay DESC\n",
    "\"\"\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4d67433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|333  |ABE   |ATL        |Long Delays  |\n",
      "|305  |ABE   |ATL        |Long Delays  |\n",
      "|275  |ABE   |ATL        |Long Delays  |\n",
      "|257  |ABE   |ATL        |Long Delays  |\n",
      "|247  |ABE   |ATL        |Long Delays  |\n",
      "|247  |ABE   |DTW        |Long Delays  |\n",
      "|219  |ABE   |ORD        |Long Delays  |\n",
      "|211  |ABE   |ATL        |Long Delays  |\n",
      "|197  |ABE   |DTW        |Long Delays  |\n",
      "|192  |ABE   |ORD        |Long Delays  |\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#A more complicated query in SQL, let's label all US flights originating from airports with high, medium, low, no delays, regardless of destinations.\n",
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "              CASE\n",
    "                  WHEN delay > 360 THEN 'Very Long Delays'\n",
    "                  WHEN delay > 120 AND delay < 360 THEN  'Long Delays '\n",
    "                  WHEN delay > 60 AND delay < 120 THEN  'Short Delays'\n",
    "                  WHEN delay > 0 and delay < 60  THEN   'Tolerable Delays'\n",
    "                  WHEN delay = 0 THEN 'No Delays'\n",
    "                  ELSE 'No Delays'\n",
    "               END AS Flight_Delays\n",
    "               FROM us_delay_flights_tbl\n",
    "               ORDER BY origin, delay DESC\"\"\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "690ede18",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29a56e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO have access to hive metastore in jupyter notebook or any ide\n",
    "spark = SparkSession.builder.appName(\"my_app\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "210f55ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('DROP DATABASE IF EXISTS learn_spark_db CASCADE ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35c2bd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE if not exists learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a66c2f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('DROP TABLE IF EXISTS managed_us_delay_flights_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfcfaba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT,distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38e361ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating table same using dataframe API\n",
    "# Path to our US flight delays CSV file\n",
    "csv_file = r\"C:\\Users\\syed3\\Downloads\\LearningSparkV2-master\\LearningSparkV2-master\\databricks-datasets\\learning-spark-v2\\flights\\departuredelays.csv\"\n",
    "# Schema as defined in the preceding example\n",
    "schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "flights_df = spark.read.csv(csv_file, schema=schema)\n",
    "flights_df.write.saveAsTable(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f6f410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary view for san francisco and newyork\n",
    "df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'JFK'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5725695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary and global temporary view\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "665aa32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|date    |delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|03010900|-11  |JFK   |LAX        |\n",
      "|03011200|-3   |JFK   |LAX        |\n",
      "|03010655|-3   |JFK   |LAX        |\n",
      "|03011030|47   |JFK   |LAX        |\n",
      "|03011900|50   |JFK   |LAX        |\n",
      "|03010800|10   |JFK   |LAX        |\n",
      "|03011700|1    |JFK   |LAS        |\n",
      "|03010800|-4   |JFK   |SFO        |\n",
      "|03011540|-3   |JFK   |DFW        |\n",
      "|03011710|3    |JFK   |SAN        |\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"us_origin_airport_JFK_tmp_view\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06533ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01011250|   55|   SFO|        JFK|\n",
      "|01012230|    0|   SFO|        JFK|\n",
      "|01010705|   -7|   SFO|        JFK|\n",
      "|01010620|   -3|   SFO|        MIA|\n",
      "|01010915|   -3|   SFO|        LAX|\n",
      "|01011005|   -8|   SFO|        DFW|\n",
      "|01011800|    0|   SFO|        ORD|\n",
      "|01011740|   -7|   SFO|        LAX|\n",
      "|01012015|   -7|   SFO|        LAX|\n",
      "|01012110|   -1|   SFO|        MIA|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Spark creates global temporaryviews in a global temporary database called global_temp\n",
    "spark.sql(\"SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80bbe7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('DROP VIEW IF EXISTS us_origin_airport_SFO_global_tmp_view')\n",
    "spark.sql('DROP VIEW IF EXISTS us_origin_airport_JFK_tmp_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "094cd934",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7758cdfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='file:/C:/Users/syed3/Downloads/Python/Python/Chapter04/spark-warehouse'),\n",
       " Database(name='learn_spark_db', description='', locationUri='file:/C:/Users/syed3/Downloads/Python/Python/Chapter04/spark-warehouse/learn_spark_db.db')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#VIEWING THE METADATA INSIDE THE DATABASE,TABLE\n",
    "spark.catalog.listDatabases()\n",
    "# spark.catalog.listTables()\n",
    "# spark.catalog.listColumns(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de5773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
